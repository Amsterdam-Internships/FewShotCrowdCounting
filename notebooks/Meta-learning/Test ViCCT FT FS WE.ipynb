{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.ViCCT.ViCCTModels  # Need to register the models!\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViCCT_small'  # Must be something like 'ViCCT_small'.\n",
    "trained_model_path = 'notebooks\\\\TL\\\\save_state_ep_250_new_best_MAE_4.971.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 3000  # The label factor used to train this specific model.\n",
    "dataset = 'WE_ViCCT_Meta'  # Must be the exact name of the dataset\n",
    "save_results = False  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!\n",
    "\n",
    "all_adapt_lrs = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]  # The learning rates which to use for \n",
    "# all_adapt_imgs = [\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_007550.jpg'], \n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg'], \n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_007550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_016550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_172550.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_141050.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_142550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_078050.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_144050.jpg']\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "adapt_imgs = [\n",
    "    [\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_007550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_090050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_172550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_069050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_078050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_169550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_091550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_007550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_141050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_003050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_076550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_041450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_011750.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'\n",
    "    ],\n",
    "    [\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_016550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_057050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_142550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_102050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_144050.jpg'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = importlib.import_module(f'datasets.meta.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.meta.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loaders, val_loaders, test_loaders, restore_transform = dataloader(adapt_imgs)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloaders = val_loaders\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloaders = test_loaders\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'ViCCT' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_optim(adapt_lr):\n",
    "    model = create_model(\n",
    "            model_name,\n",
    "            init_path=None,\n",
    "            num_classes=1000,  # Not yet used anyway. Must match pretrained model!\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,  \n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    resume_state = torch.load(trained_model_path)\n",
    "    model.load_state_dict(resume_state['net'])\n",
    "    \n",
    "    optim = torch.optim.SGD(model.parameters(), lr=adapt_lr)\n",
    "    \n",
    "    return model, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_scene(model, scene_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gts = []\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(scene_dataloader):\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "#             img = restore_transform(img)\n",
    "\n",
    "            pred_den = model(img_patches)  # Precicted density crops\n",
    "            pred_den = pred_den.cpu()\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "            \n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            preds.append(pred_cnt)\n",
    "            gts.append(gt_cnt)\n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "#             print(f'IMG {idx:<3} '\n",
    "#                   f'Prediction: {pred_cnt:<9.3f} '\n",
    "#                   f'GT: {gt_cnt:<9.3f} '\n",
    "#                   f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "#                   f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "#             if save_path:\n",
    "#                 plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return preds, gts, MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_to_scene(model, scene_dataloader, optim):\n",
    "    model.train()\n",
    "    \n",
    "    img_stack, gt_stack = scene_dataloader.dataset.get_adapt_batch()\n",
    "    img_stack, gt_stack = img_stack.squeeze(0).cuda(), gt_stack.squeeze(0).cuda()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    pred_stack = model.forward(img_stack)\n",
    "    loss = loss_fn(pred_stack, gt_stack)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim = load_model_and_optim(1.)  # Learning rate is not used when not adapting\n",
    "scene_dataloaders, restore_transform, cfg_data = get_dataloaders(None)\n",
    "for idx, scene_dataloader in enumerate(scene_dataloaders):\n",
    "    print(f'scene {idx + 1}')\n",
    "    preds_before, gts, MAE_before, MSE_before = eval_on_scene(model, scene_dataloader)\n",
    "    print(f'  No adapt MAE: {MAE_before:.3f}, MSE: {MSE_before:.3f}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_adapt_params = product(all_adapt_imgs, all_adapt_lrs)\n",
    "# for adapt_imgs, adapt_lr in all_adapt_params:\n",
    "#     print(f'lr={adapt_lr}')\n",
    "    \n",
    "#     scene_dataloaders, restore_transform, cfg_data = get_dataloaders(adapt_imgs)\n",
    "#     for idx, scene_dataloader in enumerate(scene_dataloaders):\n",
    "#         print(f'  scene {idx + 1}')\n",
    "#         model, optim = load_model_and_optim(adapt_lr)\n",
    "\n",
    "#         model = adapt_to_scene(model, scene_dataloader, optim)\n",
    "\n",
    "#         preds_after, gts, MAE_after, MSE_after = eval_on_scene(model, scene_dataloader)\n",
    "#         print(f'    After adapt -->  MAE: {MAE_after:.3f}, MSE: {MSE_after:.3f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for scene_idx in range(5):\n",
    "    print(f'Scene {scene_idx + 1}')\n",
    "    for idx, adapt_lr in enumerate(all_adapt_lrs):\n",
    "        print(f'  lr={adapt_lr}')\n",
    "\n",
    "        scene_dataloader = my_dataloaders[scene_idx]\n",
    "        model, optim = load_model_and_optim(adapt_lr)\n",
    "\n",
    "        model = adapt_to_scene(model, scene_dataloader, optim)\n",
    "\n",
    "        preds_after, gts, MAE_after, MSE_after = eval_on_scene(model, scene_dataloader)\n",
    "        print(f'    After adapt MAE/MSE: {MAE_after:.3f}/{MSE_after:.3f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_adapt_params = product(all_adapt_imgs, all_adapt_lrs)\n",
    "# for idx, (adapt_imgs, adapt_lr) in enumerate(all_adapt_params):\n",
    "#     print(f'adapt lr: {adapt_lr}')\n",
    "#     for adapt_img in adapt_imgs:\n",
    "#         print(adapt_img)\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
