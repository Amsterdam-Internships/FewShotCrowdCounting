{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.ViCCT.ViCCTModels  # Need to register the models!\n",
    "import models.ViCCT.ViCCTModelsFunctional  # Need to register the models!\n",
    "\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViCCT_small'  # Must be something like 'ViCCT_small'.\n",
    "trained_model_path = 'D:\\\\OneDrive\\\\OneDrive - UvA\\\\ThesisData\\\\trained_models\\\\ViCCT ML WE Fewshot\\\\save_state_ep_1575_new_best_MAE_5.996.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 3000  # The label factor used to train this specific model.\n",
    "dataset = 'WE_ViCCT_Meta'  # Must be the exact name of the dataset\n",
    "save_results = True  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!\n",
    "\n",
    "all_adapt_lrs = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]  # The learning rates which to use for \n",
    "# all_adapt_imgs = [\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_007550.jpg'], \n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg'], \n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_007550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_016550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_172550.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_141050.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_142550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_078050.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_144050.jpg']\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "adapt_imgs = [\n",
    "    [\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_007550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_090050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_172550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_069050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_078050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_169550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_091550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_007550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_141050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_003050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_076550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_041450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_011750.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'\n",
    "    ],\n",
    "    [\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_016550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_057050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_142550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_102050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_144050.jpg'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = importlib.import_module(f'datasets.meta.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.meta.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loaders, val_loaders, test_loaders, restore_transform = dataloader(adapt_imgs)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloaders = val_loaders\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloaders = test_loaders\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = None\n",
    "if save_results:\n",
    "    save_folder = 'ViCCT_meta' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_dir = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_optim():\n",
    "    model = create_model(\n",
    "            model_name,\n",
    "            init_path=None,\n",
    "            num_classes=1000,  # Not yet used anyway. Must match pretrained model!\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,  \n",
    "            drop_block_rate=None,\n",
    "        )\n",
    "    \n",
    "    model.make_alpha(42)\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    resume_state = torch.load(trained_model_path)\n",
    "    model.load_state_dict(resume_state['net'])\n",
    "    \n",
    "#     model.make_alpha(42)\n",
    "#     model.cuda()\n",
    "    \n",
    "    model_functional = create_model(\n",
    "        model_name + '_functional',\n",
    "        init_path=None,\n",
    "        num_classes=1000,  # Must match pretrained model!\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.,\n",
    "        drop_block_rate=None,\n",
    "    )\n",
    "\n",
    "    \n",
    "    return model, model_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scene_graph(preds_before, preds_after, gts, save_name):\n",
    "    MAE_before = np.mean(np.abs(np.array(preds_before) - np.array(gts)))\n",
    "    MAE_after = np.mean(np.abs(np.array(preds_after) - np.array(gts)))\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    xs = np.arange(len(gts))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(f'MAE before: {MAE_before:.3f}, MAE after: {MAE_after:.3f}, MAE improvement: {MAE_before - MAE_after:.3f}')\n",
    "    plt.plot(xs, gts, color='green', label='GT')\n",
    "    plt.plot(xs, preds_before, color='blue', label='Before')\n",
    "    plt.plot(xs, preds_after, '--', color='red', label='After')\n",
    "    plt.legend()\n",
    "#     plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_scene(model_funct, model_weights, scene_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gts = []\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(scene_dataloader):\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "#             img = restore_transform(img)\n",
    "\n",
    "            pred_den = model_funct.forward(img_patches, model_weights, training=False)  # Precicted density crops\n",
    "            pred_den = pred_den.cpu()\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "            \n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            preds.append(pred_cnt)\n",
    "            gts.append(gt_cnt)\n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "#             print(f'IMG {idx:<3} '\n",
    "#                   f'Prediction: {pred_cnt:<9.3f} '\n",
    "#                   f'GT: {gt_cnt:<9.3f} '\n",
    "#                   f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "#                   f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "#             if save_path:\n",
    "#                 plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return preds, gts, MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_to_scene(model, scene_dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    theta = OrderedDict((name, param) for name, param in model.named_parameters())\n",
    "    theta_values = list(theta[k] for k in theta if not k.startswith('alpha.'))\n",
    "    theta_names = list(k for k in theta if not k.startswith('alpha.'))\n",
    "    alpha_values = list(theta[k] for k in theta if k.startswith('alpha.'))\n",
    "        \n",
    "    img_stack, gt_stack = scene_dataloader.dataset.get_adapt_batch()\n",
    "    img_stack, gt_stack = img_stack.squeeze(0).cuda(), gt_stack.squeeze(0).cuda()\n",
    "\n",
    "    \n",
    "    pred = model_functional.forward(img_stack, theta, training=True)\n",
    "    loss = loss_fn(pred, gt_stack)\n",
    "    \n",
    "    grads = torch.autograd.grad(loss, theta_values)\n",
    "\n",
    "    theta_prime = OrderedDict(\n",
    "        (n, w - a * g) for n, w, a, g in zip(theta_names, theta_values, alpha_values, grads)\n",
    "    )\n",
    "    \n",
    "    return theta_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for scene_idx in range(5):\n",
    "    print(f'Scene {scene_idx + 1}')\n",
    "    \n",
    "    scene_dataloader = my_dataloaders[scene_idx]\n",
    "\n",
    "    model, model_functional = load_model_and_optim()\n",
    "    theta = OrderedDict((name, param) for name, param in model.named_parameters())\n",
    "\n",
    "    preds_before, gts, MAE_before, MSE_before = eval_on_scene(model_functional, theta, scene_dataloader)\n",
    "    print(f'    Before adapt MAE/MSE: {MAE_before:.3f}/{MSE_before:.3f}')\n",
    "\n",
    "    adapted_weights = adapt_to_scene(model, scene_dataloader)\n",
    "\n",
    "    preds_after, gts, MAE_after, MSE_after = eval_on_scene(model_functional, adapted_weights, scene_dataloader)\n",
    "    print(f'    After adapt MAE/MSE: {MAE_after:.3f}/{MSE_after:.3f}')\n",
    "\n",
    "    save_scene_graph(preds_before, preds_after, gts, f'ViCCT_WE_ML_FS_scene_{scene_idx + 1}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_adapt_params = product(all_adapt_imgs, all_adapt_lrs)\n",
    "# for idx, (adapt_imgs, adapt_lr) in enumerate(all_adapt_params):\n",
    "#     print(f'adapt lr: {adapt_lr}')\n",
    "#     for adapt_img in adapt_imgs:\n",
    "#         print(adapt_img)\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
