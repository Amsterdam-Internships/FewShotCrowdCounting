{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from itertools import product\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from models.CSRNet.CSRNet import CSRNet\n",
    "from models.CSRNet.CSRNet_functional import CSRNet_functional\n",
    "\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = 'notebooks\\\\TL\\\\save_state_ep_560_new_best_MAE_7.454.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 100  # The label factor used to train this specific model.\n",
    "dataset = 'WE_CSRNet_Meta'  # Must be the exact name of the dataset\n",
    "save_results = False  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!\n",
    "\n",
    "all_adapt_lrs = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]  # The learning rates which to use for \n",
    "# all_adapt_imgs = [\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_007550.jpg'], \n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg'], \n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_007550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_016550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_172550.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_141050.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_142550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_078050.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_144050.jpg']\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "adapt_imgs = [\n",
    "    [\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_007550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_090050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_172550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_069050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_078050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_169550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_091550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_007550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_141050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_003050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_076550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_041450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_011750.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'\n",
    "    ],\n",
    "    [\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_016550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_057050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_142550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_102050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_144050.jpg'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = importlib.import_module(f'datasets.meta.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.meta.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loaders, val_loaders, test_loaders, restore_transform = dataloader(adapt_imgs)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloaders = val_loaders\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloaders = test_loaders\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_optim(adapt_lr):\n",
    "    model = CSRNet()\n",
    "\n",
    "    resume_state = torch.load(trained_model_path)\n",
    "\n",
    "    # new_dict = {}\n",
    "    # for k, v in resume_state.items():\n",
    "    #     k = k[4:]\n",
    "    #     new_dict[k] = v\n",
    "    # model.load_state_dict(new_dict)\n",
    "\n",
    "    model.load_state_dict(resume_state['net'])\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    optim = torch.optim.SGD(model.parameters(), lr=adapt_lr)\n",
    "    \n",
    "    return model, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'CSRNet' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_scene(model, scene_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gts = []\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, gt) in enumerate(scene_dataloader):\n",
    "            img = img.cuda()\n",
    "           \n",
    "            den = model(img)  # Precicted density crops\n",
    "            den = den.cpu()\n",
    "\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "#             img = restore_transform(img.squeeze())  # Original image\n",
    "            pred_cnt = den.sum() / cfg_data.LABEL_FACTOR\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            preds.append(pred_cnt.item())\n",
    "            gts.append(gt_cnt.item())\n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "#             print(f'IMG {idx:<3} '\n",
    "#                   f'Prediction: {pred_cnt:<9.3f} '\n",
    "#                   f'GT: {gt_cnt:<9.3f} '\n",
    "#                   f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "#                   f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "#             if save_path:\n",
    "#                 plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return preds, gts, MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_to_scene(model, scene_dataloader, optim):\n",
    "    model.train()\n",
    "    \n",
    "    imgs, gts = scene_dataloader.dataset.get_adapt_batch()\n",
    "    imgs, gts = imgs.cuda(), gts.cuda()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    preds = model.forward(imgs)\n",
    "    preds = preds.squeeze(1) # remove channel dim\n",
    "    loss = loss_fn(preds, gts)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optim = load_model_and_optim(1.)  # Learning rate is not used when not adapting\n",
    "for idx, scene_dataloader in enumerate(my_dataloaders):\n",
    "    print(f'scene {idx + 1}')\n",
    "    preds_before, gts, MAE_before, MSE_before = eval_on_scene(model, scene_dataloader)\n",
    "    print(f'  No adapt MAE: {MAE_before:.3f}, MSE: {MSE_before:.3f}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene_idx in range(5):\n",
    "    print(f'Scene {scene_idx + 1}')\n",
    "    for idx, adapt_lr in enumerate(all_adapt_lrs):\n",
    "        print(f'  lr={adapt_lr}')\n",
    "\n",
    "        scene_dataloader = my_dataloaders[scene_idx]\n",
    "        model, optim = load_model_and_optim(adapt_lr)\n",
    "\n",
    "        model = adapt_to_scene(model, scene_dataloader, optim)\n",
    "\n",
    "        preds_after, gts, MAE_after, MSE_after = eval_on_scene(model, scene_dataloader)\n",
    "        print(f'    After adapt MAE/MSE: {MAE_after:.3f}/{MSE_after:.3f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
