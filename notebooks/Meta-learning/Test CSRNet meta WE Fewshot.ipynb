{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\PycharmProjects\\ThesisMain\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from models.CSRNet.CSRNet import CSRNet\n",
    "from models.CSRNet.CSRNet_functional import CSRNet_functional\n",
    "\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Parameters\n",
    "Here we define the DeiT model that we wish to evaluate and the corresponding parameters to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model_path = 'notebooks\\\\TL\\\\save_state_ep_560_new_best_MAE_7.454.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "trained_model_path = 'D:\\\\OneDrive\\\\OneDrive - UvA\\\\ThesisData\\\\trained_models\\\\CSRNet ML WE Oneshot\\\\save_state_ep_2725_new_best_MAE_4.841.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "\n",
    "label_factor = 100  # The label factor used to train this specific model.\n",
    "dataset = 'WE_CSRNet_Meta'  # Must be the exact name of the dataset\n",
    "save_results = True  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!\n",
    "\n",
    "# all_adapt_imgs = [\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_007550.jpg'], \n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg'], \n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_007550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_016550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_172550.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_141050.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_142550.jpg']\n",
    "#     ],\n",
    "#     [\n",
    "#         ['104207_1-04-S20100821071000000E20100821120000000_078050.jpg'],\n",
    "#         ['200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'],\n",
    "#         ['200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'],\n",
    "#         ['202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'],\n",
    "#         ['500717_D11-03-S20100717083000000E20100717233000000_144050.jpg']\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "adapt_imgs = [\n",
    "    [\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_007550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_090050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_172550.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_069050.jpg',\n",
    "        '104207_1-04-S20100821071000000E20100821120000000_078050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_021050.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_169550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_061550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_091550.jpg',\n",
    "        '200608_C08-02-S20100626083000000E20100626233000000_clip1_159050.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_007550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_141050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_003050.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_076550.jpg',\n",
    "        '200702_C09-01-S20100717083000000E20100717233000000_091550.jpg'\n",
    "        \n",
    "    ],\n",
    "    [\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_004550.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_041450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_023450.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_011750.jpg',\n",
    "        '202201_1-01-S20100922060000000E20100922235959000_clip1_064850.jpg'\n",
    "    ],\n",
    "    [\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_016550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_057050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_142550.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_102050.jpg',\n",
    "        '500717_D11-03-S20100717083000000E20100717233000000_144050.jpg'\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for evaluation\n",
    "Use the settings to load the DeiT model and dataloader for the test set. Also loads the transform with which we can restore the original images. Cuda is required!\n",
    "If save_results is True, also create the directory in which the predictions are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 train scenes found.\n",
      "114 test images found.\n",
      "115 test images found.\n",
      "115 test images found.\n",
      "115 test images found.\n",
      "115 test images found.\n"
     ]
    }
   ],
   "source": [
    "dataloader = importlib.import_module(f'datasets.meta.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.meta.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loaders, val_loaders, test_loaders, restore_transform = dataloader(adapt_imgs)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloaders = val_loaders\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloaders = test_loaders\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_optim():\n",
    "    model = CSRNet()\n",
    "#     model.make_alpha(42)  # Whatever, will be overwritten anyway\n",
    "    model.cuda()\n",
    "    state_dict = torch.load(trained_model_path)\n",
    "    model.make_alpha(42)  # Whatever, will be overwritten anyway\n",
    "    net = state_dict['net']\n",
    "    model.load_state_dict(net)\n",
    "\n",
    "#     model.make_alpha(42)  # Whatever, will be overwritten anyway\n",
    "    model.cuda()\n",
    "    \n",
    "    model_functional = CSRNet_functional()\n",
    "    model.eval()\n",
    "        \n",
    "    return model, model_functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = None\n",
    "if save_results:\n",
    "    save_folder = 'CSRNet_meta' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_dir = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop and save funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_scene(model_funct, model_weights, scene_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        gts = []\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, gt) in enumerate(scene_dataloader):\n",
    "            img = img.cuda()\n",
    "           \n",
    "            den = model_functional.forward(img, model_weights, training=False)  # Precicted density crops\n",
    "            den = den.cpu()\n",
    "\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "#             img = restore_transform(img.squeeze())  # Original image\n",
    "            pred_cnt = den.sum() / cfg_data.LABEL_FACTOR\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            preds.append(pred_cnt.item())\n",
    "            gts.append(gt_cnt.item())\n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "#             print(f'IMG {idx:<3} '\n",
    "#                   f'Prediction: {pred_cnt:<9.3f} '\n",
    "#                   f'GT: {gt_cnt:<9.3f} '\n",
    "#                   f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "#                   f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "#             if save_path:\n",
    "#                 plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return preds, gts, MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_to_scene(model, scene_dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    theta = OrderedDict((name, param) for name, param in model.named_parameters())\n",
    "    theta_values = list(theta[k] for k in theta if not k.startswith('alpha.'))\n",
    "    theta_names = list(k for k in theta if not k.startswith('alpha.'))\n",
    "    alpha_values = list(theta[k] for k in theta if k.startswith('alpha.'))\n",
    "\n",
    "\n",
    "    adapt_img, adapt_gt = scene_dataloader.dataset.get_adapt_batch()\n",
    "    adapt_img = adapt_img.cuda()\n",
    "    adapt_gt = adapt_gt.squeeze().cuda()\n",
    "\n",
    "    pred = model_functional.forward(adapt_img, theta, training=True)\n",
    "    pred = pred.squeeze()\n",
    "\n",
    "    loss = loss_fn(pred, adapt_gt)\n",
    "    grads = torch.autograd.grad(loss, theta_values)\n",
    "\n",
    "    theta_prime = OrderedDict(\n",
    "        (n, w - a * g) for n, w, a, g in zip(theta_names, theta_values, alpha_values, grads)\n",
    "    )\n",
    "    \n",
    "    return theta_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scene_graph(preds_before, preds_after, gts, save_name):\n",
    "    MAE_before = np.mean(np.abs(np.array(preds_before) - np.array(gts)))\n",
    "    MAE_after = np.mean(np.abs(np.array(preds_after) - np.array(gts)))\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    xs = np.arange(len(gts))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title(f'MAE before: {MAE_before:.3f}, MAE after: {MAE_after:.3f}, MAE improvement: {MAE_before - MAE_after:.3f}')\n",
    "    plt.plot(xs, gts, color='green', label='GT')\n",
    "    plt.plot(xs, preds_before, color='blue', label='Before')\n",
    "    plt.plot(xs, preds_after, '--', color='red', label='After')\n",
    "    plt.legend()\n",
    "#     plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, model_functional = load_model_and_optim()  # Learning rate is not used when not adapting\n",
    "# theta = OrderedDict((name, param) for name, param in model.named_parameters())\n",
    "\n",
    "# scene_dataloaders, restore_transform, cfg_data = get_dataloaders(None)\n",
    "# for idx, scene_dataloader in enumerate(scene_dataloaders):\n",
    "#     print(f'scene {idx + 1}')\n",
    "#     preds_before, gts, MAE_before, MSE_before = eval_on_scene(model_functional, theta, scene_dataloader)\n",
    "#     print(f'  No adapt MAE: {MAE_before:.3f}, MSE: {MSE_before:.3f}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\Anaconda3\\envs\\ThesisMain\\lib\\site-packages\\torch\\nn\\modules\\container.py:552: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Before adapt MAE/MSE: 1.775/2.293\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 2.66 GiB already allocated; 58.10 MiB free; 2.76 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3f569c91897e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'    Before adapt MAE/MSE: {MAE_before:.3f}/{MSE_before:.3f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         continue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0madapted_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapt_to_scene\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscene_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpreds_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE_after\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_on_scene\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_functional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapted_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscene_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3c4813f2c522>\u001b[0m in \u001b[0;36madapt_to_scene\u001b[1;34m(model, scene_dataloader)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0madapt_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapt_gt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_functional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapt_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\ThesisMain\\models\\CSRNet\\CSRNet_functional.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, weights, training)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frontend.17.weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frontend.17.bias'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frontend.19.weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frontend.19.bias'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 2.66 GiB already allocated; 58.10 MiB free; 2.76 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for scene_idx in range(5):\n",
    "    print(f'Scene {scene_idx + 1}')\n",
    "\n",
    "    scene_dataloader = my_dataloaders[scene_idx]\n",
    "\n",
    "    model, model_functional = load_model_and_optim()\n",
    "    theta = OrderedDict((name, param) for name, param in model.named_parameters())\n",
    "\n",
    "    preds_before, gts, MAE_before, MSE_before = eval_on_scene(model_functional, theta, scene_dataloader)\n",
    "    print(f'    Before adapt MAE/MSE: {MAE_before:.3f}/{MSE_before:.3f}')\n",
    "#         continue\n",
    "    adapted_weights = adapt_to_scene(model, scene_dataloader)\n",
    "\n",
    "    preds_after, gts, MAE_after, MSE_after = eval_on_scene(model_functional, adapted_weights, scene_dataloader)\n",
    "    print(f'    After adapt MAE/MSE: {MAE_after:.3f}/{MSE_after:.3f}')\n",
    "\n",
    "    save_scene_graph(preds_before, preds_after, gts, f'CSRNet_WE_ML_FS_scene_{scene_idx + 1}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1, 2, 4]\n",
    "# b = [2, 1, 0]\n",
    "# np.mean(np.abs(np.array(a) - np.array(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_scene_graph(preds_before, preds_after, gts, f'scene_{scene_idx + 1}_img_crowdedness_{crowdedness_names[idx]}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdasd:  12.300\n"
     ]
    }
   ],
   "source": [
    "a = 12.3\n",
    "print(f'asdasd: {a:7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
