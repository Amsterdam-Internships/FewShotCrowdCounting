{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\PycharmProjects\\ThesisMain\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6472d69bf3d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeiT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeiTModels\u001b[0m  \u001b[1;31m# Need to register the models!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_equal_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_equal_unsplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.DeiT.DeiTModels  # Need to register the models!\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Parameters\n",
    "Here we define the DeiT model that we wish to evaluate and the corresponding parameters to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deit_base_distilled_patch16_224'  # Must be something like 'deit_small_distilled_patch16_224'.\n",
    "trained_model_path = 'notebooks\\\\TL\\\\save_state_ep_1160_new_best_MAE_4.001.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 3000  # The label factor used to train this specific model.\n",
    "dataset = 'Multiset_DeiT'  # Must be the exact name of the dataset\n",
    "save_results = True  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'val'  # val', 'test'. Which split to test the model on. 'train' does not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for evaluation\n",
    "Use the settings to load the DeiT model and dataloader for the test set. Also loads the transform with which we can restore the original images. Cuda is required!\n",
    "If save_results is True, also create the directory in which the predictions are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilledRegressionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): None\n",
       "  (head): None\n",
       "  (head_dist): None\n",
       "  (regression_head): DeiTRegressionHead(\n",
       "    (regression_head): ModuleDict(\n",
       "      (lin_scaler): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (folder): Fold(output_size=(224, 224), kernel_size=16, dilation=1, padding=0, stride=16)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(\n",
    "        model_name,\n",
    "        init_path=None,\n",
    "        num_classes=1000,  # Not yet used anyway. Must match pretrained model!\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.,  \n",
    "        drop_block_rate=None,\n",
    "    )\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "resume_state = torch.load(trained_model_path)\n",
    "model.load_state_dict(resume_state['net'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 train images found in 2 datasets.\n",
      "55 val images found in 1 datasets.\n",
      "20 test images found in 1 datasets.\n"
     ]
    }
   ],
   "source": [
    "dataloader = importlib.import_module(f'datasets.standard.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.standard.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loader, val_loader, test_loader, restore_transform = dataloader(model.crop_size)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloader = val_loader\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloader = test_loader\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'DeiT' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop and save funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, my_dataloader, show_predictions, restore_transform, label_factor, cfg_data):\n",
    "    with torch.no_grad():\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(my_dataloader):\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "            img = restore_transform(img)\n",
    "\n",
    "            pred_den = model(img_patches)  # Precicted density crops\n",
    "            pred_den = pred_den.cpu()\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "            \n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "            print(f'IMG {idx:<3} '\n",
    "                  f'Prediction: {pred_cnt:<9.3f} '\n",
    "                  f'GT: {gt_cnt:<9.3f} '\n",
    "                  f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "                  f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "            if save_path:\n",
    "                plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG 0   Prediction: 18.959    GT: 19.000    Absolute Error: 0.041     Relative Error: 0.2%\n",
      "IMG 1   Prediction: 18.162    GT: 25.000    Absolute Error: 6.838     Relative Error: 27.4%\n",
      "IMG 2   Prediction: 36.180    GT: 30.000    Absolute Error: 6.180     Relative Error: 20.6%\n",
      "IMG 3   Prediction: 14.311    GT: 17.000    Absolute Error: 2.689     Relative Error: 15.8%\n",
      "IMG 4   Prediction: 21.377    GT: 28.000    Absolute Error: 6.623     Relative Error: 23.7%\n",
      "IMG 5   Prediction: 5.086     GT: 5.000     Absolute Error: 0.086     Relative Error: 1.7%\n",
      "IMG 6   Prediction: 22.482    GT: 22.000    Absolute Error: 0.482     Relative Error: 2.2%\n",
      "IMG 7   Prediction: 22.640    GT: 32.000    Absolute Error: 9.360     Relative Error: 29.3%\n",
      "IMG 8   Prediction: 8.286     GT: 6.000     Absolute Error: 2.286     Relative Error: 38.1%\n",
      "IMG 9   Prediction: 10.499    GT: 10.000    Absolute Error: 0.499     Relative Error: 5.0%\n",
      "IMG 10  Prediction: 9.795     GT: 10.000    Absolute Error: 0.205     Relative Error: 2.0%\n",
      "IMG 11  Prediction: 2.439     GT: 4.000     Absolute Error: 1.561     Relative Error: 39.0%\n",
      "IMG 12  Prediction: 15.583    GT: 18.000    Absolute Error: 2.417     Relative Error: 13.4%\n",
      "IMG 13  Prediction: 16.289    GT: 20.000    Absolute Error: 3.711     Relative Error: 18.6%\n",
      "IMG 14  Prediction: 12.472    GT: 11.000    Absolute Error: 1.472     Relative Error: 13.4%\n",
      "IMG 15  Prediction: 7.203     GT: 13.000    Absolute Error: 5.797     Relative Error: 44.6%\n",
      "IMG 16  Prediction: 2.103     GT: 1.000     Absolute Error: 1.103     Relative Error: 110.3%\n",
      "IMG 17  Prediction: 27.758    GT: 32.000    Absolute Error: 4.242     Relative Error: 13.3%\n",
      "IMG 18  Prediction: 4.338     GT: 4.000     Absolute Error: 0.338     Relative Error: 8.4%\n",
      "IMG 19  Prediction: 17.603    GT: 17.000    Absolute Error: 0.603     Relative Error: 3.5%\n",
      "IMG 20  Prediction: 6.575     GT: 12.000    Absolute Error: 5.425     Relative Error: 45.2%\n",
      "IMG 21  Prediction: 10.234    GT: 10.000    Absolute Error: 0.234     Relative Error: 2.3%\n",
      "IMG 22  Prediction: 16.789    GT: 23.000    Absolute Error: 6.211     Relative Error: 27.0%\n",
      "IMG 23  Prediction: 12.789    GT: 14.000    Absolute Error: 1.211     Relative Error: 8.6%\n",
      "IMG 24  Prediction: 9.153     GT: 13.000    Absolute Error: 3.847     Relative Error: 29.6%\n",
      "IMG 25  Prediction: 2.946     GT: 2.000     Absolute Error: 0.946     Relative Error: 47.3%\n",
      "IMG 26  Prediction: 4.649     GT: 3.000     Absolute Error: 1.649     Relative Error: 55.0%\n",
      "IMG 27  Prediction: 9.570     GT: 11.000    Absolute Error: 1.430     Relative Error: 13.0%\n",
      "IMG 28  Prediction: 45.060    GT: 45.000    Absolute Error: 0.060     Relative Error: 0.1%\n",
      "IMG 29  Prediction: 9.079     GT: 10.000    Absolute Error: 0.921     Relative Error: 9.2%\n",
      "IMG 30  Prediction: 18.098    GT: 19.000    Absolute Error: 0.902     Relative Error: 4.7%\n",
      "IMG 31  Prediction: 24.701    GT: 23.000    Absolute Error: 1.701     Relative Error: 7.4%\n",
      "IMG 32  Prediction: 15.254    GT: 14.000    Absolute Error: 1.254     Relative Error: 9.0%\n",
      "IMG 33  Prediction: 29.044    GT: 26.000    Absolute Error: 3.044     Relative Error: 11.7%\n",
      "IMG 34  Prediction: 9.038     GT: 8.000     Absolute Error: 1.038     Relative Error: 13.0%\n",
      "IMG 35  Prediction: 19.261    GT: 23.000    Absolute Error: 3.739     Relative Error: 16.3%\n",
      "IMG 36  Prediction: 23.021    GT: 28.000    Absolute Error: 4.979     Relative Error: 17.8%\n",
      "IMG 37  Prediction: 6.732     GT: 5.000     Absolute Error: 1.732     Relative Error: 34.6%\n",
      "IMG 38  Prediction: 27.433    GT: 30.000    Absolute Error: 2.567     Relative Error: 8.6%\n",
      "IMG 39  Prediction: 21.750    GT: 24.000    Absolute Error: 2.250     Relative Error: 9.4%\n",
      "IMG 40  Prediction: 6.150     GT: 9.000     Absolute Error: 2.850     Relative Error: 31.7%\n",
      "IMG 41  Prediction: 1.099     GT: 1.000     Absolute Error: 0.099     Relative Error: 9.9%\n",
      "IMG 42  Prediction: 16.557    GT: 19.000    Absolute Error: 2.443     Relative Error: 12.9%\n",
      "IMG 43  Prediction: 11.935    GT: 12.000    Absolute Error: 0.065     Relative Error: 0.5%\n",
      "IMG 44  Prediction: 3.593     GT: 5.000     Absolute Error: 1.407     Relative Error: 28.1%\n",
      "IMG 45  Prediction: 12.116    GT: 14.000    Absolute Error: 1.884     Relative Error: 13.5%\n",
      "IMG 46  Prediction: 18.720    GT: 21.000    Absolute Error: 2.280     Relative Error: 10.9%\n",
      "IMG 47  Prediction: 15.607    GT: 18.000    Absolute Error: 2.393     Relative Error: 13.3%\n",
      "IMG 48  Prediction: 10.179    GT: 12.000    Absolute Error: 1.821     Relative Error: 15.2%\n",
      "IMG 49  Prediction: 26.543    GT: 31.000    Absolute Error: 4.457     Relative Error: 14.4%\n",
      "IMG 50  Prediction: 13.513    GT: 22.000    Absolute Error: 8.487     Relative Error: 38.6%\n",
      "IMG 51  Prediction: 27.220    GT: 36.000    Absolute Error: 8.780     Relative Error: 24.4%\n",
      "IMG 52  Prediction: 1.181     GT: 1.000     Absolute Error: 0.181     Relative Error: 18.1%\n",
      "IMG 53  Prediction: 11.889    GT: 9.000     Absolute Error: 2.889     Relative Error: 32.1%\n",
      "IMG 54  Prediction: 11.764    GT: 21.000    Absolute Error: 9.236     Relative Error: 44.0%\n",
      "MAE/MSE: 2.744/3.726\n"
     ]
    }
   ],
   "source": [
    "MAE, MSE = eval_model(model, my_dataloader, save_path, restore_transform, label_factor, cfg_data)\n",
    "print(f'MAE/MSE: {MAE:.3f}/{MSE:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
