{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\PycharmProjects\\ThesisMain\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.DeiT.DeiTModels  # Need to register the models!\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Parameters\n",
    "Here we define the DeiT model that we wish to evaluate and the corresponding parameters to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deit_base_distilled_patch16_224'  # Must be something like 'deit_small_distilled_patch16_224'.\n",
    "trained_model_path = 'notebooks\\\\TL\\\\04-15_22-26\\\\state_dicts\\\\save_state_ep_1160_new_best_MAE_4.001.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 3000  # The label factor used to train this specific model.\n",
    "dataset = 'Multiset_DeiT'  # Must be the exact name of the dataset\n",
    "save_results = True  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'val'  # val', 'test'. Which split to test the model on. 'train' does not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for evaluation\n",
    "Use the settings to load the DeiT model and dataloader for the test set. Also loads the transform with which we can restore the original images. Cuda is required!\n",
    "If save_results is True, also create the directory in which the predictions are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilledRegressionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): None\n",
       "  (head): None\n",
       "  (head_dist): None\n",
       "  (regression_head): DeiTRegressionHead(\n",
       "    (regression_head): ModuleDict(\n",
       "      (lin_scaler): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (folder): Fold(output_size=(224, 224), kernel_size=16, dilation=1, padding=0, stride=16)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(\n",
    "        model_name,\n",
    "        init_path=None,\n",
    "        num_classes=1000,  # Not yet used anyway. Must match pretrained model!\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.,  \n",
    "        drop_block_rate=None,\n",
    "    )\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "resume_state = torch.load(trained_model_path)\n",
    "model.load_state_dict(resume_state['net'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 train images found in 2 datasets.\n",
      "80 val images found in 1 datasets.\n",
      "20 test images found in 1 datasets.\n"
     ]
    }
   ],
   "source": [
    "dataloader = importlib.import_module(f'datasets.standard.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.standard.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loader, val_loader, test_loader, restore_transform = dataloader(model.crop_size)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloader = val_loader\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloader = test_loader\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'DeiT' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop and save funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, my_dataloader, show_predictions, restore_transform, label_factor, cfg_data):\n",
    "    with torch.no_grad():\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(my_dataloader):\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "            img = restore_transform(img)\n",
    "\n",
    "            pred_den = model(img_patches)  # Precicted density crops\n",
    "            pred_den = pred_den.cpu()\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "            \n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "            print(f'IMG {idx:<3} '\n",
    "                  f'Prediction: {pred_cnt:<9.3f} '\n",
    "                  f'GT: {gt_cnt:<9.3f} '\n",
    "                  f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "                  f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "            if save_path:\n",
    "                plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG 0   Prediction: 13.492    GT: 11.000    Absolute Error: 2.492     Relative Error: 22.7%\n",
      "IMG 1   Prediction: 13.044    GT: 13.000    Absolute Error: 0.044     Relative Error: 0.3%\n",
      "IMG 2   Prediction: 5.039     GT: 4.000     Absolute Error: 1.039     Relative Error: 26.0%\n",
      "IMG 3   Prediction: 43.752    GT: 44.000    Absolute Error: 0.248     Relative Error: 0.6%\n",
      "IMG 4   Prediction: 7.057     GT: 5.552     Absolute Error: 1.506     Relative Error: 27.1%\n",
      "IMG 5   Prediction: 33.894    GT: 28.000    Absolute Error: 5.894     Relative Error: 21.0%\n",
      "IMG 6   Prediction: 4.042     GT: 3.000     Absolute Error: 1.042     Relative Error: 34.7%\n",
      "IMG 7   Prediction: 14.377    GT: 13.000    Absolute Error: 1.377     Relative Error: 10.6%\n",
      "IMG 8   Prediction: 5.668     GT: 3.000     Absolute Error: 2.668     Relative Error: 88.9%\n",
      "IMG 9   Prediction: 5.875     GT: 3.000     Absolute Error: 2.875     Relative Error: 95.8%\n",
      "IMG 10  Prediction: 1.342     GT: 0.000     Absolute Error: 1.342     Relative Error: inf%\n",
      "IMG 11  Prediction: 22.326    GT: 19.000    Absolute Error: 3.326     Relative Error: 17.5%\n",
      "IMG 12  Prediction: 195.008   GT: 214.000   Absolute Error: 18.992    Relative Error: 8.9%\n",
      "IMG 13  Prediction: 42.907    GT: 42.000    Absolute Error: 0.907     Relative Error: 2.2%\n",
      "IMG 14  Prediction: 103.559   GT: 105.964   Absolute Error: 2.405     Relative Error: 2.3%\n",
      "IMG 15  Prediction: 37.871    GT: 34.000    Absolute Error: 3.871     Relative Error: 11.4%\n",
      "IMG 16  Prediction: 140.053   GT: 118.000   Absolute Error: 22.053    Relative Error: 18.7%\n",
      "IMG 17  Prediction: 3.685     GT: 2.000     Absolute Error: 1.685     Relative Error: 84.3%\n",
      "IMG 18  Prediction: 86.872    GT: 80.000    Absolute Error: 6.872     Relative Error: 8.6%\n",
      "IMG 19  Prediction: 130.871   GT: 131.652   Absolute Error: 0.780     Relative Error: 0.6%\n",
      "IMG 20  Prediction: 82.128    GT: 81.000    Absolute Error: 1.128     Relative Error: 1.4%\n",
      "IMG 21  Prediction: 34.238    GT: 32.000    Absolute Error: 2.238     Relative Error: 7.0%\n",
      "IMG 22  Prediction: 5.273     GT: 2.000     Absolute Error: 3.273     Relative Error: 163.6%\n",
      "IMG 23  Prediction: 82.987    GT: 89.986    Absolute Error: 6.999     Relative Error: 7.8%\n",
      "IMG 24  Prediction: 10.894    GT: 10.000    Absolute Error: 0.894     Relative Error: 8.9%\n",
      "IMG 25  Prediction: 78.451    GT: 71.000    Absolute Error: 7.451     Relative Error: 10.5%\n",
      "IMG 26  Prediction: 9.468     GT: 8.000     Absolute Error: 1.468     Relative Error: 18.4%\n",
      "IMG 27  Prediction: 6.148     GT: 4.000     Absolute Error: 2.148     Relative Error: 53.7%\n",
      "IMG 28  Prediction: 25.152    GT: 25.000    Absolute Error: 0.152     Relative Error: 0.6%\n",
      "IMG 29  Prediction: 41.079    GT: 41.000    Absolute Error: 0.079     Relative Error: 0.2%\n",
      "IMG 30  Prediction: 30.860    GT: 36.000    Absolute Error: 5.140     Relative Error: 14.3%\n",
      "IMG 31  Prediction: 31.073    GT: 29.986    Absolute Error: 1.087     Relative Error: 3.6%\n",
      "IMG 32  Prediction: 8.759     GT: 9.000     Absolute Error: 0.241     Relative Error: 2.7%\n",
      "IMG 33  Prediction: 1.589     GT: 0.000     Absolute Error: 1.589     Relative Error: inf%\n",
      "IMG 34  Prediction: 67.218    GT: 64.000    Absolute Error: 3.218     Relative Error: 5.0%\n",
      "IMG 35  Prediction: 22.570    GT: 27.000    Absolute Error: 4.430     Relative Error: 16.4%\n",
      "IMG 36  Prediction: 39.222    GT: 35.000    Absolute Error: 4.222     Relative Error: 12.1%\n",
      "IMG 37  Prediction: 32.450    GT: 31.000    Absolute Error: 1.450     Relative Error: 4.7%\n",
      "IMG 38  Prediction: 2.850     GT: 2.000     Absolute Error: 0.850     Relative Error: 42.5%\n",
      "IMG 39  Prediction: 21.695    GT: 25.000    Absolute Error: 3.305     Relative Error: 13.2%\n",
      "IMG 40  Prediction: 16.936    GT: 17.000    Absolute Error: 0.064     Relative Error: 0.4%\n",
      "IMG 41  Prediction: 20.407    GT: 21.743    Absolute Error: 1.336     Relative Error: 6.1%\n",
      "IMG 42  Prediction: 11.163    GT: 10.000    Absolute Error: 1.163     Relative Error: 11.6%\n",
      "IMG 43  Prediction: 33.963    GT: 30.000    Absolute Error: 3.963     Relative Error: 13.2%\n",
      "IMG 44  Prediction: 205.396   GT: 227.894   Absolute Error: 22.498    Relative Error: 9.9%\n",
      "IMG 45  Prediction: 46.361    GT: 40.000    Absolute Error: 6.361     Relative Error: 15.9%\n",
      "IMG 46  Prediction: 9.984     GT: 7.000     Absolute Error: 2.984     Relative Error: 42.6%\n",
      "IMG 47  Prediction: 26.773    GT: 27.000    Absolute Error: 0.227     Relative Error: 0.8%\n",
      "IMG 48  Prediction: 150.652   GT: 130.000   Absolute Error: 20.652    Relative Error: 15.9%\n",
      "IMG 49  Prediction: 6.070     GT: 7.000     Absolute Error: 0.930     Relative Error: 13.3%\n",
      "IMG 50  Prediction: 110.558   GT: 108.766   Absolute Error: 1.792     Relative Error: 1.6%\n",
      "IMG 51  Prediction: 55.369    GT: 50.000    Absolute Error: 5.369     Relative Error: 10.7%\n",
      "IMG 52  Prediction: 27.289    GT: 25.000    Absolute Error: 2.289     Relative Error: 9.2%\n",
      "IMG 53  Prediction: 34.301    GT: 32.000    Absolute Error: 2.301     Relative Error: 7.2%\n",
      "IMG 54  Prediction: 121.516   GT: 123.000   Absolute Error: 1.484     Relative Error: 1.2%\n",
      "IMG 55  Prediction: 27.083    GT: 26.000    Absolute Error: 1.083     Relative Error: 4.2%\n",
      "IMG 56  Prediction: 11.637    GT: 11.000    Absolute Error: 0.637     Relative Error: 5.8%\n",
      "IMG 57  Prediction: 14.616    GT: 13.000    Absolute Error: 1.616     Relative Error: 12.4%\n",
      "IMG 58  Prediction: 10.059    GT: 8.000     Absolute Error: 2.059     Relative Error: 25.7%\n",
      "IMG 59  Prediction: 188.183   GT: 205.820   Absolute Error: 17.638    Relative Error: 8.6%\n",
      "IMG 60  Prediction: 46.987    GT: 42.000    Absolute Error: 4.987     Relative Error: 11.9%\n",
      "IMG 61  Prediction: 21.542    GT: 19.000    Absolute Error: 2.542     Relative Error: 13.4%\n",
      "IMG 62  Prediction: 99.738    GT: 99.000    Absolute Error: 0.738     Relative Error: 0.7%\n",
      "IMG 63  Prediction: 175.750   GT: 175.203   Absolute Error: 0.547     Relative Error: 0.3%\n",
      "IMG 64  Prediction: 48.028    GT: 49.820    Absolute Error: 1.793     Relative Error: 3.6%\n",
      "IMG 65  Prediction: 149.883   GT: 137.743   Absolute Error: 12.140    Relative Error: 8.8%\n",
      "IMG 66  Prediction: 45.352    GT: 43.000    Absolute Error: 2.352     Relative Error: 5.5%\n",
      "IMG 67  Prediction: 19.071    GT: 16.000    Absolute Error: 3.071     Relative Error: 19.2%\n",
      "IMG 68  Prediction: 18.267    GT: 19.000    Absolute Error: 0.733     Relative Error: 3.9%\n",
      "IMG 69  Prediction: 32.509    GT: 36.000    Absolute Error: 3.491     Relative Error: 9.7%\n",
      "IMG 70  Prediction: 3.053     GT: 1.000     Absolute Error: 2.053     Relative Error: 205.3%\n",
      "IMG 71  Prediction: 20.526    GT: 24.000    Absolute Error: 3.474     Relative Error: 14.5%\n",
      "IMG 72  Prediction: 133.731   GT: 134.000   Absolute Error: 0.269     Relative Error: 0.2%\n",
      "IMG 73  Prediction: 10.299    GT: 12.930    Absolute Error: 2.632     Relative Error: 20.4%\n",
      "IMG 74  Prediction: 3.629     GT: 2.000     Absolute Error: 1.629     Relative Error: 81.5%\n",
      "IMG 75  Prediction: 15.635    GT: 12.000    Absolute Error: 3.635     Relative Error: 30.3%\n",
      "IMG 76  Prediction: 1.537     GT: 0.000     Absolute Error: 1.537     Relative Error: inf%\n",
      "IMG 77  Prediction: 12.126    GT: 9.000     Absolute Error: 3.126     Relative Error: 34.7%\n",
      "IMG 78  Prediction: 10.814    GT: 11.000    Absolute Error: 0.186     Relative Error: 1.7%\n",
      "IMG 79  Prediction: 35.701    GT: 37.000    Absolute Error: 1.299     Relative Error: 3.5%\n",
      "MAE/MSE: 3.518/5.964\n"
     ]
    }
   ],
   "source": [
    "MAE, MSE = eval_model(model, my_dataloader, save_path, restore_transform, label_factor, cfg_data)\n",
    "print(f'MAE/MSE: {MAE:.3f}/{MSE:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
