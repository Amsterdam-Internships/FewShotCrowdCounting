{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\PycharmProjects\\ThesisMain\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.DeiT.DeiTModels  # Need to register the models!\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and Parameters\n",
    "Here we define the DeiT model that we wish to evaluate and the corresponding parameters to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deit_base_distilled_patch16_224'  # Must be something like 'deit_small_distilled_patch16_224'.\n",
    "trained_model_path = 'notebooks/to_arena_05/save_state_ep_330_new_best_MAE_2.649.pth'  # The path to trained model file (something like XYZ.pth)\n",
    "label_factor = 10000  # The label factor used to train this specific model.\n",
    "dataset = 'Multiset_DeiT'  # Must be the exact name of the dataset\n",
    "save_results = True  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for evaluation\n",
    "Use the settings to load the DeiT model and dataloader for the test set. Also loads the transform with which we can restore the original images. Cuda is required!\n",
    "If save_results is True, also create the directory in which the predictions are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilledRegressionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): None\n",
       "  (head): None\n",
       "  (head_dist): None\n",
       "  (regression_head): DeiTRegressionHead(\n",
       "    (regression_head): ModuleDict(\n",
       "      (lin_scaler): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (folder): Fold(output_size=(224, 224), kernel_size=16, dilation=1, padding=0, stride=16)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model(\n",
    "        model_name,\n",
    "        init_path=None,\n",
    "        num_classes=1000,  # Not yet used anyway. Must match pretrained model!\n",
    "        drop_rate=0.,\n",
    "        drop_path_rate=0.1,  # TODO: What does this do?\n",
    "        drop_block_rate=None,\n",
    "    )\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "resume_state = torch.load(trained_model_path)\n",
    "model.load_state_dict(resume_state['net'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 train images found in 2 datasets.\n",
      "62 val images found in 1 datasets.\n",
      "20 test images found in 1 datasets.\n"
     ]
    }
   ],
   "source": [
    "dataloader = importlib.import_module(f'datasets.standard.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.standard.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loader, val_loader, test_loader, restore_transform = dataloader(model.crop_size)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloader = val_loader\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloader = test_loader\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'DeiT' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop and save funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, my_dataloader, show_predictions, restore_transform, label_factor, cfg_data):\n",
    "    with torch.no_grad():\n",
    "        AEs = []  # Absolute Errors\n",
    "        SEs = []  # Squared Errors\n",
    "\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(my_dataloader):\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "            img = restore_transform(img)\n",
    "\n",
    "            pred_den = model(img_patches)  # Precicted density crops\n",
    "            pred_den = pred_den.cpu()\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "\n",
    "            \n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "            print(f'IMG {idx:<3} '\n",
    "                  f'Prediction: {pred_cnt:<9.3f} '\n",
    "                  f'GT: {gt_cnt:<9.3f} '\n",
    "                  f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "                  f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "            if save_path:\n",
    "                plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "\n",
    "    return MAE, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG 0   Prediction: 24.984    GT: 23.000    Absolute Error: 1.984     Relative Error: 8.6%\n",
      "IMG 1   Prediction: 36.340    GT: 32.000    Absolute Error: 4.340     Relative Error: 13.6%\n",
      "IMG 2   Prediction: 27.224    GT: 26.000    Absolute Error: 1.224     Relative Error: 4.7%\n",
      "IMG 3   Prediction: 34.028    GT: 41.000    Absolute Error: 6.972     Relative Error: 17.0%\n",
      "IMG 4   Prediction: 22.553    GT: 24.000    Absolute Error: 1.447     Relative Error: 6.0%\n",
      "IMG 5   Prediction: 30.453    GT: 26.000    Absolute Error: 4.453     Relative Error: 17.1%\n",
      "IMG 6   Prediction: 22.167    GT: 21.000    Absolute Error: 1.167     Relative Error: 5.6%\n",
      "IMG 7   Prediction: 17.964    GT: 22.000    Absolute Error: 4.036     Relative Error: 18.3%\n",
      "IMG 8   Prediction: 18.039    GT: 16.000    Absolute Error: 2.039     Relative Error: 12.7%\n",
      "IMG 9   Prediction: 20.456    GT: 18.000    Absolute Error: 2.456     Relative Error: 13.6%\n",
      "IMG 10  Prediction: 26.665    GT: 26.000    Absolute Error: 0.665     Relative Error: 2.6%\n",
      "IMG 11  Prediction: 25.250    GT: 30.000    Absolute Error: 4.750     Relative Error: 15.8%\n",
      "IMG 12  Prediction: 22.462    GT: 28.000    Absolute Error: 5.538     Relative Error: 19.8%\n",
      "IMG 13  Prediction: 14.447    GT: 17.000    Absolute Error: 2.553     Relative Error: 15.0%\n",
      "IMG 14  Prediction: 24.857    GT: 29.000    Absolute Error: 4.143     Relative Error: 14.3%\n",
      "IMG 15  Prediction: 6.350     GT: 6.000     Absolute Error: 0.350     Relative Error: 5.8%\n",
      "IMG 16  Prediction: 4.132     GT: 1.000     Absolute Error: 3.132     Relative Error: 313.2%\n",
      "IMG 17  Prediction: 6.560     GT: 5.000     Absolute Error: 1.560     Relative Error: 31.2%\n",
      "IMG 18  Prediction: 18.551    GT: 18.000    Absolute Error: 0.551     Relative Error: 3.1%\n",
      "IMG 19  Prediction: 10.551    GT: 10.000    Absolute Error: 0.551     Relative Error: 5.5%\n",
      "MAE: 2.696     Root MSE: 3.263\n"
     ]
    }
   ],
   "source": [
    "MAE, MSE = eval_model(model, my_dataloader, save_path, restore_transform, label_factor, cfg_data)\n",
    "print(f'MAE: {MAE:<9.3f} Root MSE: {MSE:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
